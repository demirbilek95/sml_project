{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# check if machine has gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path that training set is located\n",
    "path = \"../data/fruits/fruits-360/\"\n",
    "# this joins the path + folder and each files e.g. '../data/fruits/fruits-360/Training/Apple Braeburn/115_100.jpg'\n",
    "files_training = glob(os.path.join(path,'Training', '*/*.jpg'))\n",
    "num_images = len(files_training)\n",
    "print('Number of images in Training file:', num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just to see how many images we have for each label, minimum one and average one, with nice printing style\n",
    "\n",
    "min_images = 1000\n",
    "im_cnt = []\n",
    "class_names = []\n",
    "print('{:18s}'.format('class'), end='')\n",
    "print('Count:')\n",
    "print('-' * 24)\n",
    "for folder in os.listdir(os.path.join(path, 'Training')):\n",
    "    folder_num = len(os.listdir(os.path.join(path,'Training',folder)))\n",
    "    im_cnt.append(folder_num)\n",
    "    class_names.append(folder)\n",
    "    print('{:20s}'.format(folder), end=' ')\n",
    "    print(folder_num)\n",
    "        \n",
    "num_classes = len(class_names)\n",
    "print(\"\\nMinumum images per category:\", np.min(im_cnt), 'Category:', class_names[im_cnt.index(np.min(im_cnt))])    \n",
    "print('Average number of Images per Category: {:.0f}'.format(np.array(im_cnt).mean()))\n",
    "print('Total number of classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to guess pop_mean and pop_std\n",
    "\n",
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "training_data = ImageFolder(os.path.join(path, 'Training'), tensor_transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(training_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# this part takes a bit long\n",
    "pop_mean = [0.6840367,0.5786325,0.5037564]  # normally it was --> []\n",
    "pop_std = [0.30334985,0.3599262,0.3913685]\n",
    "\n",
    "# for i, data in tqdm(enumerate(data_loader, 0)):\n",
    "#     numpy_image = data[0].numpy()\n",
    "    \n",
    "#     batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "#     batch_std = np.std(numpy_image, axis=(0,2,3))\n",
    "    \n",
    "#     pop_mean.append(batch_mean)\n",
    "#     pop_std.append(batch_std)\n",
    "\n",
    "# pop_mean = np.array(pop_mean).mean(axis=0)\n",
    "# pop_std = np.array(pop_std).mean(axis=0)\n",
    "\n",
    "# that is why I am inserting last values \n",
    "\n",
    "print(pop_mean)\n",
    "print(pop_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "shuffle = np.random.permutation(num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation images\n",
    "\n",
    "split_val = int(num_images * 0.2)\n",
    "print('Total number of images:', num_images)\n",
    "print('Number images in validation set:',len(shuffle[:split_val]))\n",
    "print('Number images in train set:',len(shuffle[split_val:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitTrainDataset(Dataset):\n",
    "    def __init__(self, files, shuffle, split_val, class_names, transform=transforms.ToTensor()):\n",
    "        self.shuffle = shuffle\n",
    "        self.class_names = class_names\n",
    "        self.split_val = split_val\n",
    "        self.data = np.array([files[i] for i in shuffle[split_val:]])\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx])\n",
    "        name = self.data[idx].split('/')[-2]\n",
    "        y = self.class_names.index(name)\n",
    "        img = self.transform(img)\n",
    "            \n",
    "        return img, y\n",
    "\n",
    "class FruitValidDataset(Dataset):\n",
    "    def __init__(self, files, shuffle, split_val, class_names, transform=transforms.ToTensor()):\n",
    "        self.shuffle = shuffle\n",
    "        self.class_names = class_names\n",
    "        self.split_val = split_val\n",
    "        self.data = np.array([files[i] for i in shuffle[:split_val]])\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx])\n",
    "        name = self.data[idx].split('/')[-2]\n",
    "        y = self.class_names.index(name)\n",
    "        img = self.transform(img)\n",
    "            \n",
    "        return img, y\n",
    "    \n",
    "class FruitTestDataset(Dataset):\n",
    "    def __init__(self, path, class_names, transform=transforms.ToTensor()):\n",
    "        self.class_names = class_names\n",
    "        self.data = np.array(glob(os.path.join(path, '*/*.jpg')))\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx])\n",
    "        name = self.data[idx].split('/')[-2]\n",
    "        y = self.class_names.index(name)\n",
    "        img = self.transform(img)\n",
    "            \n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(pop_mean, pop_std) # These were the mean and standard deviations that we calculated earlier.\n",
    "    ]),\n",
    "    'Test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(pop_mean, pop_std) # These were the mean and standard deviations that we calculated earlier.\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(pop_mean, pop_std) # These were the mean and standard deviations that we calculated earlier.\n",
    "    ])\n",
    "}\n",
    "\n",
    "train_dataset = FruitTrainDataset(files_training, shuffle, split_val, class_names, data_transforms['train'])\n",
    "valid_dataset = FruitValidDataset(files_training, shuffle, split_val, class_names, data_transforms['valid'])\n",
    "test_dataset = FruitTestDataset(\"../data/fruits/fruits-360/Test\", class_names, transform=data_transforms['Test'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_loader,\n",
    "              'valid': valid_loader,\n",
    "              'Test': test_loader}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'valid': len(valid_dataset),\n",
    "    'Test': len(test_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = pop_std * inp + pop_mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(train_loader))\n",
    "out = make_grid(inputs)\n",
    "\n",
    "cats = ['' for x in range(len(classes))]\n",
    "for i in range(len(classes)):\n",
    "    cats[i] = class_names[classes[i].item()]\n",
    "    \n",
    "imshow(out)\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to check if shape of train and test sets match\n",
    "for i,j in zip(train_loader,test_loader):\n",
    "    print(i[0].shape,j[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=30000, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=131, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# just to start from the basic NN and to observe how does it perform on data\n",
    "# with horizontal and vertical flip we have 3x100x100\n",
    "# batch size was 64 adn reduced to 32 to get better performance\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # initialize the parent class methods\n",
    "        self.fc1 = nn.Linear(3*100*100, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 131)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move network to GPU\n",
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's train the network, # regular way of training with in sample accuracy\n",
    "def train(net):\n",
    "    for epoch in tqdm(range(10)):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i,data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.view(-1,3*100*100))\n",
    "            \n",
    "            # in sample accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            a = predicted == labels\n",
    "            correct += np.count_nonzero(a.cpu())\n",
    "            total += len(a)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f, in sample accuracy: %.3f' %(epoch, i + 1, running_loss / 100, correct/total))\n",
    "                running_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                \n",
    "    print('Finished Training')\n",
    "    \n",
    "train(net)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fnn_net is the first model\n",
    "# second one is different arch.\n",
    "# third one is the batch size is 32 instead of 64\n",
    "\n",
    "PATH = \"../models/fnn_net_3.pth\"\n",
    "torch.save(net.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images.view(-1,3*100*100))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label wise accuracy\n",
    "class_correct = list(0. for i in range(131))\n",
    "class_total = list(0. for i in range(131))\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images.view(-1,3*100*100))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(131):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "    class_names[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to show how models can be load\n",
    "\n",
    "PATH = \"../models/fnn_net_3.pth\"\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In sample accuracy can be added to train function\n",
    "    * It is added\n",
    "\n",
    "* Plots will be done (loss + in_sample_accuracy etc) for model analysis\n",
    "    * Now I am on this stage, validation acc and loss will be calculated also plots will be done\n",
    "\n",
    "* This network will be extended (maybe adding new layer or changing other parameters)\n",
    "    * This matter tried and saved as a second model, but results didn't change too much and training set took more time, so we can exclude this option. As a last trial, batch size is changed from 64 to 32 and in this case network worked better and total accuracy increased to %86\n",
    "\n",
    "* Other architecture (especially CNN and its variants) will be tested"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
